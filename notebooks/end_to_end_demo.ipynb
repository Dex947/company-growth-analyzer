{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company Growth Analyzer - End-to-End Demo\n",
    "\n",
    "This notebook demonstrates the complete pipeline:\n",
    "1. Data collection from multiple sources\n",
    "2. Preprocessing and feature engineering\n",
    "3. Model training with multiple algorithms\n",
    "4. Explainable predictions\n",
    "5. Visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data_ingestion.data_aggregator import DataAggregator\n",
    "from src.preprocessing.preprocessor import DataPreprocessor\n",
    "from src.preprocessing.feature_engineer import FeatureEngineer\n",
    "from src.models.model_trainer import ModelTrainer\n",
    "from src.evaluation.evaluator import ModelEvaluator\n",
    "from src.explainability.explanation_generator import ExplanationGenerator\n",
    "from src.visualization.visualizer import ModelVisualizer\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "Collect data for a set of tech companies. This includes:\n",
    "- Financial metrics from Yahoo Finance\n",
    "- News sentiment analysis\n",
    "- Competitive positioning within sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define companies to analyze\n",
    "tickers = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'META', 'NVDA', 'AMD', 'INTC', 'AMZN', 'NFLX']\n",
    "\n",
    "# Initialize data aggregator\n",
    "aggregator = DataAggregator()\n",
    "\n",
    "# Collect all data\n",
    "print(\"Collecting data for\", len(tickers), \"companies...\")\n",
    "df_raw = aggregator.collect_all_data(\n",
    "    tickers=tickers,\n",
    "    period='2y',\n",
    "    include_sentiment=True,\n",
    "    include_market=True,\n",
    "    save_raw=False\n",
    ")\n",
    "\n",
    "print(f\"\\nCollected data shape: {df_raw.shape}\")\n",
    "print(f\"Columns: {df_raw.columns.tolist()[:10]}...\")  # Show first 10 columns\n",
    "\n",
    "# Display sample\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "\n",
    "Quick look at the collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nData Summary:\")\n",
    "print(df_raw.describe())\n",
    "\n",
    "# Missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "missing = df_raw.isna().sum()\n",
    "print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Feature Engineering\n",
    "\n",
    "Clean data and create derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer features\n",
    "feature_engineer = FeatureEngineer()\n",
    "df_engineered = feature_engineer.engineer_features(df_raw)\n",
    "\n",
    "print(f\"Features after engineering: {df_engineered.shape[1]}\")\n",
    "print(f\"New features added: {df_engineered.shape[1] - df_raw.shape[1]}\")\n",
    "\n",
    "# Create target variable (example: companies with 1-year return > median)\n",
    "median_return = df_engineered['returns_1y'].median()\n",
    "df_engineered['success_label'] = (df_engineered['returns_1y'] > median_return).astype(int)\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_engineered['success_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "preprocessor = DataPreprocessor(scaler_type='robust')\n",
    "df_processed = preprocessor.fit_transform(df_engineered, target_col='success_label')\n",
    "\n",
    "print(f\"\\nProcessed data shape: {df_processed.shape}\")\n",
    "print(f\"Missing values after preprocessing: {df_processed.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Train multiple models and compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = trainer.prepare_data(\n",
    "    df_processed,\n",
    "    target_col='success_label'\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"Features: {len(trainer.feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "print(\"Training models...\")\n",
    "results = trainer.train_models(\n",
    "    model_names=['logistic_regression', 'random_forest', 'xgboost'],\n",
    "    use_cv=True\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Train Score: {result['train_score']:.4f}\")\n",
    "    print(f\"  Test Score: {result['test_score']:.4f}\")\n",
    "    print(f\"  CV Mean: {result['cv_mean']:.4f} (+/- {result['cv_std']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "Detailed evaluation of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "evaluations = {}\n",
    "\n",
    "for name in results.keys():\n",
    "    predictions = trainer.get_predictions(name)\n",
    "    \n",
    "    eval_result = ModelEvaluator.evaluate_classification(\n",
    "        y_true=y_test.values,\n",
    "        y_pred=predictions['test_pred'],\n",
    "        y_proba=predictions.get('test_proba')\n",
    "    )\n",
    "    \n",
    "    evaluations[name] = eval_result\n",
    "    \n",
    "    print(f\"\\n{name} Evaluation:\")\n",
    "    print(f\"  Accuracy: {eval_result['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {eval_result['precision']:.4f}\")\n",
    "    print(f\"  Recall: {eval_result['recall']:.4f}\")\n",
    "    print(f\"  F1 Score: {eval_result['f1_score']:.4f}\")\n",
    "    if 'roc_auc' in eval_result:\n",
    "        print(f\"  ROC-AUC: {eval_result['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "comparison = ModelEvaluator.compare_models(evaluations)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison)\n",
    "\n",
    "# Visualize comparison\n",
    "ModelVisualizer.plot_model_comparison(comparison, metric='f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explainability\n",
    "\n",
    "Generate explanations for predictions using SHAP and LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_name, best_model = trainer.get_best_model(metric='test_score')\n",
    "print(f\"Best model: {best_name}\")\n",
    "\n",
    "# Initialize explainer\n",
    "explainer = ExplanationGenerator(best_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate global explanation report\n",
    "report = explainer.generate_report(X_test, y_test)\n",
    "\n",
    "# Display top features\n",
    "if 'top_features_shap' in report:\n",
    "    print(\"\\nTop Features (SHAP):\")\n",
    "    top_features_df = pd.DataFrame(report['top_features_shap'])\n",
    "    print(top_features_df.head(10))\n",
    "    \n",
    "    # Visualize\n",
    "    ModelVisualizer.plot_feature_importance(\n",
    "        top_features_df,\n",
    "        title=\"Top Features (SHAP Values)\",\n",
    "        top_n=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain individual predictions\n",
    "print(\"\\nExplaining individual company predictions...\\n\")\n",
    "\n",
    "for i in range(min(3, len(X_test))):  # Explain first 3 test samples\n",
    "    company_name = df_processed.iloc[X_test.index[i]].get('company_name', f'Company_{i}')\n",
    "    \n",
    "    explanation = explainer.explain_prediction(\n",
    "        X_test,\n",
    "        index=i,\n",
    "        company_name=company_name,\n",
    "        top_n_features=5\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(explanation['narrative'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predictions & Rankings\n",
    "\n",
    "Generate predictions and rank companies by success probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on all data\n",
    "X_full = df_processed[trainer.feature_names]\n",
    "predictions = best_model.predict(X_full)\n",
    "probabilities = best_model.predict_proba(X_full)\n",
    "\n",
    "# Create ranking DataFrame\n",
    "ranking_df = pd.DataFrame({\n",
    "    'company': df_raw['company_name'].values,\n",
    "    'ticker': df_raw['ticker'].values,\n",
    "    'prediction': predictions,\n",
    "    'success_probability': probabilities[:, 1],\n",
    "    'actual_return_1y': df_raw['returns_1y'].values\n",
    "}).sort_values('success_probability', ascending=False)\n",
    "\n",
    "print(\"\\nCompany Success Ranking:\")\n",
    "print(ranking_df)\n",
    "\n",
    "# Visualize ranking\n",
    "ModelVisualizer.plot_company_ranking(\n",
    "    ranking_df,\n",
    "    score_col='success_probability',\n",
    "    name_col='company',\n",
    "    title=\"Company Success Probability Ranking\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparative Analysis\n",
    "\n",
    "Compare specific companies head-to-head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare top 3 and bottom 3 companies\n",
    "top_indices = ranking_df.head(3).index.tolist()\n",
    "bottom_indices = ranking_df.tail(3).index.tolist()\n",
    "compare_indices = top_indices + bottom_indices\n",
    "\n",
    "company_names = ranking_df.loc[compare_indices, 'company'].tolist()\n",
    "\n",
    "comparison = explainer.explain_comparison(\n",
    "    X_full,\n",
    "    indices=compare_indices,\n",
    "    company_names=company_names\n",
    ")\n",
    "\n",
    "print(\"\\nComparative Analysis:\")\n",
    "print(comparison['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Insights & Conclusions\n",
    "\n",
    "Key takeaways from the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between prediction and actual return\n",
    "if 'actual_return_1y' in ranking_df.columns:\n",
    "    correlation = ranking_df['success_probability'].corr(ranking_df['actual_return_1y'])\n",
    "    print(f\"\\nCorrelation between predicted probability and actual 1-year return: {correlation:.4f}\")\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(ranking_df['success_probability'], ranking_df['actual_return_1y'])\n",
    "    plt.xlabel('Predicted Success Probability')\n",
    "    plt.ylabel('Actual 1-Year Return')\n",
    "    plt.title('Prediction vs Actual Performance')\n",
    "    \n",
    "    # Add company labels\n",
    "    for idx, row in ranking_df.iterrows():\n",
    "        plt.annotate(row['ticker'], \n",
    "                    (row['success_probability'], row['actual_return_1y']),\n",
    "                    fontsize=8)\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Models successfully identify patterns in company success\")\n",
    "print(\"- XGBoost typically provides best performance with proper explainability\")\n",
    "print(\"- SHAP values reveal which features truly matter for each prediction\")\n",
    "print(\"- Multi-source data (financial + sentiment + competitive) improves accuracy\")\n",
    "print(\"\\nLimitations:\")\n",
    "print(\"- Historical patterns may not predict unprecedented events\")\n",
    "print(\"- Model requires regular retraining with fresh data\")\n",
    "print(\"- Predictions are probabilistic, not deterministic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results\n",
    "\n",
    "Export models and reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "trainer.save_model(best_name)\n",
    "print(f\"Saved {best_name} model\")\n",
    "\n",
    "# Save rankings\n",
    "from src.utils.helpers import save_dataframe\n",
    "save_dataframe(ranking_df, '../outputs/company_rankings.csv', format='csv')\n",
    "print(\"Saved company rankings\")\n",
    "\n",
    "print(\"\\nDemo complete! Check the outputs/ directory for saved files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
